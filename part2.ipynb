{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Average\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import pickle\n",
    "import itertools\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "# Personal libraries\n",
    "import henon_map as hm\n",
    "\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Radial average:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "computing full track:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A/home/carlidel/Insync/carlo.montanari3@studio.unibo.it/OneDrive Biz/optimized_code/henon_map/henon_map/cpu_henon_core.py:153: RuntimeWarning: invalid value encountered in true_divide\n",
      "  matrices = np.nansum(matrices.reshape(\n",
      "\n",
      "computing full track:   2%|▎         | 1/40 [01:28<57:33, 88.55s/it]\u001b[A\n",
      "computing full track:   5%|▌         | 2/40 [02:50<54:47, 86.50s/it]\u001b[A\n",
      "computing full track:   8%|▊         | 3/40 [04:10<52:05, 84.48s/it]\u001b[A\n",
      "computing full track:  10%|█         | 4/40 [05:27<49:30, 82.52s/it]\u001b[A\n",
      "computing full track:  12%|█▎        | 5/40 [06:41<46:29, 79.70s/it]\u001b[A\n",
      "computing full track:  15%|█▌        | 6/40 [07:51<43:33, 76.88s/it]\u001b[A\n",
      "computing full track:  18%|█▊        | 7/40 [08:59<40:45, 74.11s/it]\u001b[A\n",
      "computing full track:  20%|██        | 8/40 [10:06<38:25, 72.04s/it]\u001b[A\n",
      "computing full track:  22%|██▎       | 9/40 [11:11<36:09, 69.97s/it]\u001b[A\n",
      "computing full track:  25%|██▌       | 10/40 [12:13<33:44, 67.49s/it]\u001b[A\n",
      "computing full track:  28%|██▊       | 11/40 [13:12<31:25, 65.02s/it]\u001b[A\n",
      "computing full track:  30%|███       | 12/40 [14:10<29:26, 63.10s/it]\u001b[A\n",
      "computing full track:  32%|███▎      | 13/40 [15:07<27:26, 60.99s/it]\u001b[A\n",
      "computing full track:  35%|███▌      | 14/40 [15:59<25:21, 58.52s/it]\u001b[A\n",
      "computing full track:  38%|███▊      | 15/40 [16:51<23:32, 56.51s/it]\u001b[A\n",
      "computing full track:  40%|████      | 16/40 [17:40<21:44, 54.34s/it]\u001b[A\n",
      "computing full track:  42%|████▎     | 17/40 [18:27<19:58, 52.12s/it]\u001b[A\n",
      "computing full track:  45%|████▌     | 18/40 [19:12<18:16, 49.86s/it]\u001b[A\n",
      "computing full track:  48%|████▊     | 19/40 [19:55<16:46, 47.94s/it]\u001b[A\n",
      "computing full track:  50%|█████     | 20/40 [20:37<15:17, 45.89s/it]\u001b[A\n",
      "computing full track:  52%|█████▎    | 21/40 [21:15<13:47, 43.57s/it]\u001b[A\n",
      "computing full track:  55%|█████▌    | 22/40 [21:50<12:21, 41.18s/it]\u001b[A\n",
      "computing full track:  57%|█████▊    | 23/40 [22:24<11:00, 38.87s/it]\u001b[A\n",
      "computing full track:  60%|██████    | 24/40 [22:56<09:50, 36.93s/it]\u001b[A\n",
      "computing full track:  62%|██████▎   | 25/40 [23:26<08:42, 34.81s/it]\u001b[A\n",
      "computing full track:  65%|██████▌   | 26/40 [23:54<07:38, 32.73s/it]\u001b[A\n",
      "computing full track:  68%|██████▊   | 27/40 [24:20<06:38, 30.68s/it]\u001b[A\n",
      "computing full track:  70%|███████   | 28/40 [24:43<05:42, 28.58s/it]\u001b[A\n",
      "computing full track:  72%|███████▎  | 29/40 [25:05<04:50, 26.40s/it]\u001b[A\n",
      "computing full track:  75%|███████▌  | 30/40 [25:25<04:04, 24.46s/it]\u001b[A\n",
      "computing full track:  78%|███████▊  | 31/40 [25:43<03:22, 22.52s/it]\u001b[A\n",
      "computing full track:  80%|████████  | 32/40 [25:59<02:45, 20.65s/it]\u001b[A\n",
      "computing full track:  82%|████████▎ | 33/40 [26:13<02:10, 18.68s/it]\u001b[A\n",
      "computing full track:  85%|████████▌ | 34/40 [26:25<01:40, 16.75s/it]\u001b[A\n",
      "computing full track:  88%|████████▊ | 35/40 [26:36<01:14, 14.87s/it]\u001b[A\n",
      "computing full track:  90%|█████████ | 36/40 [26:45<00:52, 13.04s/it]\u001b[A\n",
      "computing full track:  92%|█████████▎| 37/40 [26:52<00:33, 11.30s/it]\u001b[A\n",
      "computing full track:  95%|█████████▌| 38/40 [26:57<00:19,  9.56s/it]\u001b[A\n",
      "computing full track:  98%|█████████▊| 39/40 [27:01<00:07,  7.76s/it]\u001b[A\n",
      "computing full track: 100%|██████████| 40/40 [27:02<00:00, 40.56s/it]\u001b[A\n",
      "Radial average: 100%|██████████| 1/1 [29:54<00:00, 1794.71s/it]\n"
     ]
    }
   ],
   "source": [
    "error_2 = {}\n",
    "DA_2 = {}\n",
    "error_5 = {}\n",
    "DA_5 = {}\n",
    "raw_error_2 = {}\n",
    "\n",
    "alpha_preliminary_values = np.linspace(-1.0, 1.0, samples)\n",
    "alpha_values = np.arccos(alpha_preliminary_values) / 2\n",
    "d_preliminar_alpha = alpha_preliminary_values[1] - alpha_preliminary_values[0]\n",
    "\n",
    "for epsilon in tqdm(epsilons, desc=\"Radial average\"):\n",
    "    # Extracting the radiuses with theta1 = theta2 = 0.0\n",
    "    \n",
    "    engine = hm.radial_scan.generate_instance(\n",
    "        d_r, \n",
    "        alpha_values, \n",
    "        np.zeros(alpha_values.shape),\n",
    "        np.zeros(alpha_values.shape),\n",
    "        epsilon,\n",
    "        starting_position=starting_position\n",
    "    )\n",
    "    all_radiuses = engine.compute(turn_sampling)\n",
    "    \n",
    "    # Computing of the 2D values\n",
    "    \n",
    "    skips = [1]\n",
    "    while True:\n",
    "        if (samples - 1) // skips[-1] > 4:\n",
    "            skips.append(skips[-1] * 2)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    for i in skips:\n",
    "        alpha_prelim = alpha_preliminary_values[::i]\n",
    "        cutted_radiuses = all_radiuses[::i]\n",
    "        value = integrate.simps(cutted_radiuses ** 2, alpha_prelim, axis=0)\n",
    "        less_value = integrate.simps(cutted_radiuses[::2] ** 2, alpha_prelim[::2], axis=0)\n",
    "        uncertainty = np.abs((value - less_value))\n",
    "        \n",
    "        DA = np.sqrt(value / 2)\n",
    "        uncertainty = 0.5 * np.power(value / 2, -0.5) * uncertainty\n",
    "        DA_5[(epsilon, cutted_radiuses.shape)] = np.asarray(DA)\n",
    "        error_5[(epsilon, cutted_radiuses.shape)] = uncertainty \n",
    "    \n",
    "    # Rest of the Angular Averaging process\n",
    "    \n",
    "    values = []\n",
    "    indexes = []\n",
    "    refined_values = []\n",
    "    skipper = 256\n",
    "    for i in tqdm(range(len(turn_sampling)), desc=\"computing full track\"):\n",
    "        temp_values = np.array([[]])\n",
    "        for index, j in enumerate(range(0, samples, skipper)):\n",
    "            stopping = (j + skipper if j != samples - 1 else samples)\n",
    "            radiuses = all_radiuses[j : stopping, i]\n",
    "\n",
    "            engine = hm.full_track.generate_instance(\n",
    "                radiuses,\n",
    "                alpha_values[j : stopping],\n",
    "                np.zeros(alpha_values.shape)[j : stopping],\n",
    "                np.zeros(alpha_values.shape)[j : stopping],\n",
    "                np.ones(alpha_values.shape, dtype=np.int)[j : stopping] * turn_sampling[i],\n",
    "                epsilon)\n",
    "\n",
    "            engine.compute()\n",
    "            engine.accumulate_and_return(n_subdivisions)\n",
    "            \n",
    "            if index == 0:\n",
    "                list_count_matrix = engine.count_matrix.copy()\n",
    "                list_avg_matrix = engine.matrices.copy()\n",
    "            else:\n",
    "                list_count_matrix = np.concatenate((list_count_matrix, engine.count_matrix))\n",
    "                list_avg_matrix = np.concatenate((list_avg_matrix, engine.matrices))\n",
    "        \n",
    "        _, _, result_total, validity_total = hm.cpu_accumulate_and_return(list_count_matrix, list_avg_matrix)\n",
    "        \n",
    "        values.append(result_total)\n",
    "        indexes.append(np.argmax(validity_total, axis=0))\n",
    "        refined_values.append([result_total[indexes[-1][i]][i] for i in range(len(indexes[-1]))])\n",
    "        # count_matrix_2[(epsilon, samples, turn_sampling[i])] = list_count_matrix\n",
    "        # average_matrix_2[(epsilon, samples, turn_sampling[i])] = list_avg_matrix\n",
    "    \n",
    "    steps = [1]\n",
    "    while True:\n",
    "        if (values[0][0].shape[0] - 1) / steps[-1] > 4:\n",
    "            steps.append(steps[-1] * 2)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for jump in steps:\n",
    "        DA = []\n",
    "        error = []\n",
    "        DA_mc = []\n",
    "        raw_error_mc = []\n",
    "        error_mc = []\n",
    "        for i in range(len(turn_sampling)):\n",
    "            DA.append(np.power(integrate.simps(refined_values[i][::jump], alpha_preliminary_values[::jump]) * 0.5, 1/4))\n",
    "            temp = np.power(integrate.simps(refined_values[i][::jump * 2], alpha_preliminary_values[::jump * 2]) * 0.5, 1/4)\n",
    "            error.append(np.absolute(DA[-1] - temp))\n",
    "\n",
    "            DA_mc.append(np.power(np.average(refined_values[i][::jump]), 1/4))\n",
    "            raw_error_mc.append(np.std(refined_values[i][::jump]))\n",
    "            error_mc.append(0.25 * np.power(DA_mc[-1], -3) * np.std(values[i][::jump]) / np.sqrt(np.size(values[i][::jump])))\n",
    "\n",
    "        DA_2[(epsilon, len(refined_values[i][::jump]), \"refined\", \"int\")] = DA\n",
    "        error_2[(epsilon, len(refined_values[i][::jump]), \"refined\", \"int\")] = error\n",
    "        DA_2[(epsilon, len(refined_values[i][::jump]), \"refined\", \"mc\")] = DA_mc\n",
    "        error_2[(epsilon, len(refined_values[i][::jump]), \"refined\", \"mc\")] = error_mc\n",
    "        raw_error_2[(epsilon, len(refined_values[i][::jump]), \"refined\", \"mc\")] = raw_error_mc\n",
    "            \n",
    "        for j in range(len(values[0])):\n",
    "            DA = []\n",
    "            error = []\n",
    "            DA_mc = []\n",
    "            raw_error_mc = []\n",
    "            error_mc = []\n",
    "            for i in range(len(turn_sampling)):\n",
    "                DA.append(np.power(integrate.simps(values[i][j][::jump], alpha_preliminary_values[::jump]) * 0.5, 1/4))\n",
    "                temp = np.power(integrate.simps(values[i][j][::jump * 2], alpha_preliminary_values[::jump * 2]) * 0.5, 1/4)\n",
    "                error.append(np.absolute(DA[-1] - temp))\n",
    "                \n",
    "                DA_mc.append(np.power(np.average(values[i][j][::jump]), 1/4))\n",
    "                raw_error_mc.append(np.std(values[i][j][::jump]))\n",
    "                error_mc.append(0.25 * np.power(DA_mc[-1], -3) * np.std(values[i][j][::jump]) / np.sqrt(np.size(values[i][j][::jump])))\n",
    "                \n",
    "            DA_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"int\")] = DA\n",
    "            error_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"int\")] = error\n",
    "            DA_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"mc\")] = DA_mc\n",
    "            error_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"mc\")] = error_mc\n",
    "            raw_error_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"mc\")] = raw_error_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(savepath + \"data/DA_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(DA_2, f, protocol=4)\n",
    "    \n",
    "with open(savepath + \"data/error_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(error_2, f, protocol=4)\n",
    "\n",
    "with open(savepath + \"data/raw_error_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(raw_error_2, f, protocol=4)    \n",
    "\n",
    "with open(savepath + \"data/DA_5.pkl\", 'wb') as f:\n",
    "    pickle.dump(DA_5, f, protocol=4)\n",
    "    \n",
    "with open(savepath + \"data/error_5.pkl\", 'wb') as f:\n",
    "    pickle.dump(error_5, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
