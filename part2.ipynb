{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Average\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "from tqdm import tqdm\n",
    "from scipy.special import erf\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "\n",
    "# Personal libraries\n",
    "import henon_map as hm\n",
    "\n",
    "from parameters import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Radial average:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:25,  2.81s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:05<00:22,  2.77s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:08<00:18,  2.70s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:10<00:15,  2.63s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:12<00:12,  2.55s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:15<00:09,  2.43s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:17<00:06,  2.31s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:18<00:04,  2.17s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:20<00:02,  2.02s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.20s/it]\u001b[A\n",
      "Radial average: 100%|██████████| 1/1 [00:24<00:00, 24.99s/it]\n"
     ]
    }
   ],
   "source": [
    "error_2 = {}\n",
    "DA_2 = {}\n",
    "count_matrix_2 = {}\n",
    "average_matrix_2 = {}\n",
    "raw_error_2 = {}\n",
    "\n",
    "alpha_preliminary_values = np.linspace(-1.0, 1.0, samples)\n",
    "alpha_values = np.arccos(alpha_preliminary_values) / 2\n",
    "d_preliminar_alpha = alpha_preliminary_values[1] - alpha_preliminary_values[0]\n",
    "\n",
    "for epsilon in tqdm(epsilons, desc=\"Radial average\"):\n",
    "    # Extracting the radiuses with theta1 = theta2 = 0.0\n",
    "    \n",
    "    engine = hm.radial_scan.generate_instance(\n",
    "        d_r, \n",
    "        alpha_values, \n",
    "        np.zeros(alpha_values.shape),\n",
    "        np.zeros(alpha_values.shape),\n",
    "        epsilon\n",
    "    )\n",
    "    all_radiuses = engine.compute(turn_sampling)\n",
    "    \n",
    "    values = []\n",
    "    for i in tqdm(range(len(turn_sampling))):\n",
    "        temp_values = np.array([[]])\n",
    "        for index, j in enumerate(range(0, samples, 128)):\n",
    "            stopping = (j + 128 if j != samples - 1 else samples)\n",
    "            radiuses = all_radiuses[j : stopping, i]\n",
    "\n",
    "            engine = hm.full_track.generate_instance(\n",
    "                radiuses,\n",
    "                alpha_values[j : stopping],\n",
    "                np.zeros(alpha_values.shape)[j : stopping],\n",
    "                np.zeros(alpha_values.shape)[j : stopping],\n",
    "                np.ones(alpha_values.shape, dtype=np.int)[j : stopping] * turn_sampling[i],\n",
    "                epsilon)\n",
    "\n",
    "            x, y, px, py = engine.compute()\n",
    "            \n",
    "            _ = engine.accumulate_and_return(n_subdivisions)\n",
    "            tmp_count_matrix = engine.count_matrix\n",
    "            tmp_avg_matrix = engine.matrices\n",
    "            if index == 0:\n",
    "                list_count_matrix = tmp_count_matrix\n",
    "                list_avg_matrix = tmp_avg_matrix\n",
    "            else:\n",
    "                list_count_matrix = np.concatenate((list_count_matrix, tmp_count_matrix))\n",
    "                list_avg_matrix = np.concatenate((list_avg_matrix, tmp_avg_matrix))\n",
    "        \n",
    "        count_total, matrix_total, result_total = hm.cpu_accumulate_and_return(list_count_matrix, list_avg_matrix)\n",
    "        \n",
    "        values.append(result_total)\n",
    "        count_matrix_2[(epsilon, samples, turn_sampling[i])] = count_total\n",
    "        average_matrix_2[(epsilon, samples, turn_sampling[i])] = matrix_total\n",
    "    \n",
    "    steps = [1]\n",
    "    while True:\n",
    "        if (values[0][0].shape[0] - 1) / steps[-1] > 4:\n",
    "            steps.append(steps[-1] * 2)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for jump in steps:\n",
    "        for j in range(len(values[0])):\n",
    "            DA = []\n",
    "            error = []\n",
    "            DA_mc = []\n",
    "            raw_error_mc = []\n",
    "            error_mc = []\n",
    "            for i in range(len(turn_sampling)):\n",
    "                DA.append(np.power(integrate.romb(values[i][j][::jump], d_preliminar_alpha * jump) * 0.5, 1/4))\n",
    "                temp = np.power(integrate.romb(values[i][j][::jump * 2], d_preliminar_alpha * jump * 2) * 0.5, 1/4)\n",
    "                error.append(np.absolute(DA[-1] - temp))\n",
    "                \n",
    "                DA_mc.append(np.power(np.average(values[i][j][::jump]), 1/4))\n",
    "                raw_error_mc.append(np.std(values[i][j][::jump]))\n",
    "                error_mc.append(0.25 * np.power(DA_mc[-1], -3) * np.std(values[i][j][::jump]) / np.sqrt(np.size(values[i][j][::jump])))\n",
    "                \n",
    "            DA_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"int\")] = DA\n",
    "            error_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"int\")] = error\n",
    "            DA_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"mc\")] = DA_mc\n",
    "            error_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"mc\")] = error_mc\n",
    "            raw_error_2[(epsilon, len(values[i][j][::jump]), 2 ** (j), \"mc\")] = raw_error_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/DA_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(DA_2, f, protocol=4)\n",
    "    \n",
    "with open(\"data/error_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(error_2, f, protocol=4)\n",
    "\n",
    "with open(\"data/raw_error_2.pkl\", 'wb') as f:\n",
    "    pickle.dump(raw_error_2, f, protocol=4)    \n",
    "\n",
    "# with open(\"data/count_matrix_2.pkl\", 'wb') as f:\n",
    "#     pickle.dump(count_matrix_2, f, protocol=4)\n",
    "    \n",
    "# with open(\"data/avg_matrix_2.pkl\", 'wb') as f:\n",
    "#     pickle.dump(average_matrix_2, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the end, we need to plot stuff here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:03,  1.40it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.57it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.74it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:02<00:01,  1.88it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  1.99it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.07it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:26,  2.91s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.19it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.11it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.16it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.22it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.26it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.23it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:05<00:22,  2.84s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.10it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.13it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.16it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.21it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.09it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.14it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:08<00:19,  2.83s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.05it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.10it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.14it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.16it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.19it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.21it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:11<00:16,  2.80s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.12it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.16it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.19it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.17it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.00it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.07it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:14<00:14,  2.83s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.97it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.04it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.09it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.14it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.16it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.18it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:16<00:11,  2.80s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.02it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.06it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.05it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.07it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.05it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.06it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:19<00:08,  2.84s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.94it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:01<00:02,  1.84it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  1.93it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:02<00:00,  2.02it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.08it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.05it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:22<00:05,  2.86s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.08it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.12it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.15it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.15it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.15it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.15it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:25<00:02,  2.84s/it]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  1.94it/s]\u001b[A\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.02it/s]\u001b[A\n",
      " 50%|█████     | 3/6 [00:01<00:01,  2.09it/s]\u001b[A\n",
      " 67%|██████▋   | 4/6 [00:01<00:00,  2.13it/s]\u001b[A\n",
      " 83%|████████▎ | 5/6 [00:02<00:00,  2.18it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.19it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:28<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "cmap = matplotlib.cm.get_cmap('plasma')\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "for key in tqdm(list(sorted(count_matrix_2, key=lambda a : a[2]))):\n",
    "    if key[0] == epsilon:\n",
    "        for i in tqdm(range(len(average_matrix_2[key]))):\n",
    "            fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "            coso = axs[0].imshow(np.nanmean(average_matrix_2[key][i], axis=0), origin=\"lower\", extent=(0, np.pi*2, 0, np.pi*2))\n",
    "            axs[0].set_title(\"Average radius measured\\n$\\\\varepsilon={}$, $\\\\alpha$ samples $= {}$, $N$ iters $={}$\".format(key[0], key[1], key[2]))\n",
    "            axs[0].set_xlabel(\"$\\\\theta_1$\")\n",
    "            axs[0].set_ylabel(\"$\\\\theta_2$\")\n",
    "            fig.colorbar(coso, ax=axs[0])\n",
    "\n",
    "            axs[0].xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(int(x/np.pi)) + \"$\\\\pi$\"))\n",
    "            axs[0].xaxis.set_major_locator(ticker.MultipleLocator(base=np.pi))\n",
    "            axs[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(int(x/np.pi)) + \"$\\\\pi$\"))\n",
    "            axs[0].yaxis.set_major_locator(ticker.MultipleLocator(base=np.pi))\n",
    "\n",
    "            coso = axs[1].imshow(np.nanmean(count_matrix_2[key][i], axis=0), origin=\"lower\", extent=(0, np.pi*2, 0, np.pi*2))\n",
    "            axs[1].set_title(\"Number of samples\\n$\\\\varepsilon={}$, $\\\\alpha$ samples $= {}$, $N$ iters $={}$\".format(key[0], key[1], key[2]))\n",
    "            axs[1].set_xlabel(\"$\\\\theta_1$\")\n",
    "            axs[1].set_ylabel(\"$\\\\theta_2$\")\n",
    "            fig.colorbar(coso, ax=axs[1])\n",
    "\n",
    "            axs[1].xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(int(x/np.pi)) + \"$\\\\pi$\"))\n",
    "            axs[1].xaxis.set_major_locator(ticker.MultipleLocator(base=np.pi))\n",
    "            axs[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: str(int(x/np.pi)) + \"$\\\\pi$\"))\n",
    "            axs[1].yaxis.set_major_locator(ticker.MultipleLocator(base=np.pi))\n",
    "\n",
    "            plt.tight_layout()\n",
    "            # print(key)\n",
    "            plt.savefig(\"img/cm_eps_\" + str(int(key[0])) + \"_N_\" + str(key[1]) + \"_t_\" + str(key[2]) + \"_el_\" + str(i) + \".jpg\", dpi=300)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import ffmpeg\n",
    "\n",
    "#name = \"img/movie.mp4\"\n",
    "#(\n",
    "#    ffmpeg\n",
    "#    .input('img/cm_eps*el*.jpg', pattern_type='glob', framerate=2)\n",
    "#    .output(name)\n",
    "#    .run()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
